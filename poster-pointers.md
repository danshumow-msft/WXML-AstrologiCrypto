## WXML Poster Pointers

### Background
State the problem we worked on this quarter.

### What is Mutual Info?
Mutual Information is a measure of a distance between two random variables. Informally it measures how much information one gets about one random variable after knowing the result of the other random variable. 

#### In terms of the marginals
For discrete random variables $X$ and $Y$, their mutual information is given by $$I(X;Y) = \sum_{y \in \mathcal{Y}} \sum_{x \in \mathcal{X}} P_{(X,Y)}(x,y) \cdot \log \left(\frac{P_{(X,Y)}(x,y)}{p_X(x) p_Y(y)}\right),$$

where $P_{(X,Y)}$ is the joint probability density and $p_X, p_Y$ are the marginal densities.

### Why Mutual Info?
Expound more on what you looked at last quarter, and the need for mutual information.

### How?
Expound on the setup, how much data and how the data was generated. What analysis was done etc.
Elaborate on the results we saw.
This can be graphics or you can state certain features, or patterns you observed.

### What's next?
How would one proceed next? 
